---
title: "Analysis of confidence intervals in journal abstracts"
author: "Adrian Barnett"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, comment='', dpi=400)
options(width=1000) # Wide pages
options(scipen=999) # avoid scientific presentation
# libraries
library(stringr) 
library(R2WinBUGS)
library(dplyr)
library(pander)
library(tables)
library(broom)
panderOptions('table.emphasize.rownames', FALSE)
panderOptions('keep.trailing.zeros', TRUE)
panderOptions('table.split.table', Inf)
panderOptions('table.split.cells', Inf)
panderOptions('big.mark', ',')
library(ggplot2)
g.theme = theme_bw() + theme(panel.grid.minor = element_blank())
cbPalette = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# function to round with trailing zeros
roundz  = function(x, digits=0){formatC( round( x, digits ), format='f', digits=digits)}
```

# Estimates extracted by me

## Spread of abstract over time

```{r dates}
# get the data
load('data/Analysis.Ready.RData') # from 2_data_edits.R
# plot
ylabs = 2009:2019
breaks = as.numeric(as.Date(paste(ylabs,'-01-01', sep=''), origin='1970-01-01'))
dplot = ggplot(data=any.data, aes(x=as.numeric(date)))+
  geom_histogram(fill='skyblue')+
  scale_x_continuous(breaks=breaks, labels = ylabs)+
  xlab('')+
  ylab('')+
  theme_bw()+
  g.theme
dplot
```

The plot above shows the randomly selected abstracts by date.
The total number of abstracts was `r nrow(any.data)`.

## What proportion of abstracts included confidence intervals

```{r overall.proportion}
tab = tabular(Heading('Any CI')*factor(any.ci) + 1 ~ (n=1 + Percent('col')), data=any.data)
pander(tab, digits=1)
```


## What confidence level was used

```{r multinom.ci.level, include=FALSE}
if(length(dir('data', pattern='multinomial.bugs.RData'))==0){ # if results do not exist
  # use multinomial to get CI
  model.file = 'multinomial.bugs.txt'
  bugs = file(model.file, 'w')
  cat('model{
	levels[1 : J] ~ dmulti( p[1 : J], N)
p[1:J] ~ ddirch(alpha[1:J])
}\n', file=bugs)
  close(bugs)
  pre.tab = table(data$ci.level[data$ci.level!='Missing']) # exclude missing
  tab = as.numeric(pre.tab) # exclude missing
  bdata = list(J=4, levels=tab, N=sum(tab), alpha=c(0.01,0.01,0.01,0.01))
  parms = 'p'
  bugs.results =  bugs(data=bdata, inits=NULL, parameters=parms, model.file=model.file,
                       n.chains=1, n.thin=2, n.iter=10000, debug=FALSE, DIC=FALSE,
                       bugs.directory="c:/Program Files/WinBUGS14")
  # make a nice table of the results
to.tab = data.frame(bugs.results$summary[, c(1,3,7)]*100)
names(to.tab) = c('Mean','lower','upper')
to.tab = mutate(to.tab, Level =  names(pre.tab),  
                n = pre.tab,
                Mean = roundz(Mean, 1),
                CI = paste(roundz(lower, 1), ' to ' , roundz(upper, 1), sep='')) %>%
          select('Level','n','Mean','CI')
  save(bugs.results, to.tab, file='data/multinomial.bugs.RData')
} # end of results if
if(length(dir('data', pattern='multinomial.bugs.RData'))>0){ # if results already exist then load them
  load('data/multinomial.bugs.RData')
}
pander(to.tab, style='simple')
```

```{r what.level.table}
tab = tabular(Heading('CI level')*factor(ci.level) + 1 ~ (n=1 + Percent('col')), data=data)
pander(tab, digits=1)
```

The table shows the level of confidence interval used for four options from 80% to 99%. When a level was given it was almost always 95%. A large number of intervals were given with no level.

## Types of intervals per abstract

```{r interval.types}
tab = tabular(Heading('Interval type')*factor(type) + 1 ~ (n=1 + Percent('col')), data=data)
pander(tab, digits=1)
```

## Number of intervals per abstract


```{r abstract.stats}
astats = dplyr::group_by(data, pubmed) %>%
  dplyr::mutate(count = n()) %>%
  dplyr::filter(row_number()==1) %>% # Just one results per paper
  dplyr::select(pubmed, count)
bplot = ggplot(data=astats, aes(x=count))+
  geom_bar(fill='skyblue')+
  xlab('Number of intervals')+
  ylab('Frequency')+
  theme_bw()+
  g.theme
bplot
```

The bar plot shows the number of labelled intervals per abstract for those abstracts that have at least one labelled interval.

The table below shows the abstracts with 20 or more intervals.

```{r twenty}
twenty = filter(astats, count>=20) %>%
  ungroup() %>%
  mutate(pubmed = as.character(pubmed)) %>%
  select(pubmed, count)
pander(twenty, style='simple')
```

## Number of intervals per abstract over time

Here we use a regression model to examine whether the number of intervals per abstract has increased over time.
This analysis includes those abstracts with no intervals.
We use a poisson model and allow for over-dispersion.

```{r over.time}
# merge with all data to get abstracts with no intervals
ref.date = as.numeric(as.Date('2000-01-01'))
for.model = left_join(any.data, astats, by='pubmed') %>%
  mutate(time.year = (as.numeric(date) - ref.date)/365.25) # centre date and scale to year
for.model$count[is.na(for.model$count)] = 0 # replace missing with zero
pmodel = glm(count ~ time.year, data=for.model, family=quasipoisson()) # added over-dispersion
to.table = tidy(pmodel)
to.table = mutate(to.table, p.value = format.pval(p.value, eps=0.0001))
pander(to.table, style='simple')
# for text below
inc.year = 100*(exp(to.table$estimate[2])-1) # percent increase per year
# could look at interaction between time and interval type. Have certain intervals been increasing more than others? Not that interesting!
```

There was a clear increase in the number of intervals per abstract over time. The average increase over time was `r roundz(inc.year,1)` percent per year.

## What percent of intervals were wrong?

We examined mistakes where the mean was not within the lower and upper interval.

```{r mistakes}
tab = tabular(Heading('Mistake')*factor(mistake) + 1~(n=1 + Percent('col')), data=data)
pander(tab, digits=1)
```

## How close do CI widths get to the null hypothesis border

These results exclude the intervals that were mistakes.
The cumulative plot is on the log scale.

### Plot of lower and upper intervals side by side

```{r plot.both}
# ideally would have year too
lower = filter(data, type %in% c('OR','HR','RR'), mistake==FALSE, lower>0)%>% # just these three types
  mutate(link=1:n(), itype='Lower') %>%
  select(link, type, itype, lower) %>%
  rename('interval'='lower')
upper = filter(data, type %in% c('OR','HR','RR'), upper>0)%>% 
  mutate(link=1:n(), itype='Upper') %>%
  select(link, type, itype, upper) %>%
  rename('interval'='upper')
both.intervals = bind_rows(lower, upper)
# split by CI type
bplot = ggplot(both.intervals, aes(x=interval))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  theme_bw()+
  theme(panel.grid.minor = element_blank())+
  facet_grid(type ~ itype)
bplot
# export plot
jpeg('figures/BothIntervalMe.jpg', width=5, height=6, units='in', res=400, quality=100)
print(bplot)
invisible(dev.off())
```

# Using Georgescu and Wren data

```{r load.gw, include=FALSE}
load('data/Georgescu.Wren.RData') # from1_data_from_Georgescu_Wren.R
complete = filter(complete, is.na(lower)==FALSE) # remove small number with missing lower limit
zero = sum(complete$lower==0) # count of zero intervals
zero.perc = round(zero/nrow(complete)*100, 2)
n.journals = length(unique(complete$journal)) # number of journals
```

There are `r nrow(complete)` confidence limits from `r n.journals` journals. 

There are `r zero` lower confidence limits equal to zero, which is `r zero.perc` percent of the total.
These were excluded from the plots below.

## Lower confidence interval (excluding zero)

```{r gw.lower}
# split by CI type - make nicer label (also add numbers?)
rr.lower = filter(complete, lower>0) 
lplot = ggplot(rr.lower, aes(x=lower))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  theme_bw()+
  theme(panel.grid.minor = element_blank())
lplot
# number and percent excluded from plot by the limits on the x-axis
perc.lower.ex = round(100*sum(rr.lower$lower<0.1)/nrow(rr.lower),1)
perc.upper.ex = round(100*sum(rr.lower$lower>10)/nrow(rr.lower),1)
# export plot
jpeg('figures/lowerIntervalGW.jpg', width=5, height=4, units='in', res=400, quality=100)
print(lplot)
invisible(dev.off())
```

There were `r perc.lower.ex`% limits excluded from the plot that were less than 0.1, and `r perc.upper.ex`% limits excluded that were above 10.

## Number and percent of lower intervals above 1 and less than or equal to 1.1

```{r tab.one}
for.table = filter(complete, lower > 0) %>%
  mutate(narrow = lower>1 & lower<=1.1,
         narrowf = factor(as.numeric(narrow), levels=0:1, labels=c('No','Yes')))
tab = tabular((Heading('')*narrowf+1)~((n=1) + Percent('col')), data=for.table)
pander(tab, digits=0)
```

## Number and percent of lower intervals above 1 and less than or equal to 1.2

```{r tab.one.one}
for.table = filter(complete, lower > 0) %>%
  mutate(narrow = lower>1 & lower<=1.2,
         narrowf = factor(as.numeric(narrow), levels=0:1, labels=c('No','Yes')))
tab = tabular((Heading('')*narrowf+1)~((n=1) + Percent('col')), data=for.table)
pander(tab, digits=0)
```

## Upper confidence interval 

```{r gw.upper}
uplot = ggplot(complete, aes(x=upper))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  coord_cartesian(xlim=c(0.5, 2), ylim=c(0,0.6))+ # limit x-axis to focus on key change
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  theme_bw()+  
  theme(panel.grid.minor = element_blank())
uplot
```

## Plot of lower and upper intervals side by side

```{r plot.both.gw}
n.no.year = sum(is.na(complete$Year)) # number with missing year, used below
lower = filter(complete, mistake==FALSE) %>% # exclude mistakes
  mutate(link=1:n(), itype='Lower') %>%
  select(link, Year, itype, lower) %>%
  rename('interval'='lower') 
upper = filter(complete, mistake==FALSE) %>% # exclude mistakes
  mutate(link=1:n(), itype='Upper') %>%
  select(link, Year, itype, upper) %>%
  rename('interval'='upper') 
both.intervals = bind_rows(lower, upper)
# split by CI type
bplot = ggplot(both.intervals, aes(x=interval))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  theme_bw()+
  theme(panel.grid.minor = element_blank())+
  facet_wrap( ~ itype)
bplot
# export plot
jpeg('figures/BothIntervalGW.jpg', width=5, height=3, units='in', res=400, quality=100)
print(bplot)
invisible(dev.off())
```

## Alternative plot on same panel

```{r plot.both.gw.same.panel}
# split by CI type
bplot = ggplot(both.intervals, aes(x=interval, col=factor(itype)))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  scale_color_manual('Interval', values=cbPalette[2:3])+
  theme_bw()+
  theme(panel.grid.minor = element_blank(), legend.position = c(0.2, 0.8))
bplot
# export plot
jpeg('figures/BothIntervalGWColour.jpg', width=4, height=4, units='in', res=400, quality=100)
print(bplot)
invisible(dev.off())
```

## Intervals over time 

We examine the cumulative plots over time to see whether things have worsened or improved.
We plot the empirical cumulative distribution function in five year bands.
We excluded the data prior to 1990 because of the relatively small numbers.

```{r gw.over.time, fig.width=8, fig.height=6}
# group into five year periods
ylabels = paste(seq(1990,2015,5),' to ', seq(1994,2019,5), sep='')
to.plot = filter(both.intervals, interval>0, Year >= 1990) %>% # exclude early years with little data
  mutate(five.year = floor((Year-1985)/5))
# numbers per 5 year period
numbers = group_by(to.plot, five.year) %>%
  filter(itype=='Lower') %>%  # avoid double counting
  summarise(n=n(), miny=min(Year), maxy=max(Year))
# add numbers to labels
numbers = str_replace_all(pattern=' ', replacement='', string=format(numbers$n, big.mark=','))
ylabels = paste(ylabels, ' (n = ', numbers, ')', sep='')
# add labels to data
to.plot = mutate(to.plot, five.yearc = factor(five.year, levels=1:6, labels=ylabels))
# split by CI type
lyplot = ggplot(to.plot, aes(x=interval, col=factor(itype)))+
  geom_vline(xintercept = 1, lty=2, col='grey')+
  stat_ecdf(geom = "step", size=1.1)+
  scale_x_log10(breaks=c(0.1,1,10), labels=c(0.1,1,10))+
  scale_color_manual('Interval', values=cbPalette[2:3])+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  theme_bw()+
  theme(panel.grid.minor = element_blank())+
  facet_wrap(~five.yearc)
lyplot
# export plot
jpeg('figures/IntervalsTimeGW.jpg', width=6.5, height=5, units='in', res=400, quality=100)
print(lyplot)
invisible(dev.off())
```

The plot shows a near identical cumulative distribution function for each five year period.

This plot excludes `r n.no.year` intervals that were missing the year.


## Sensitivity analysis using one result per abstract

The above results make no adjustment for using repeated data from the same abstract. Here we use a sensitivity analysis to check whehter this dependence is influencing the results. We do this by randomly selecting one interval per abstract and re-creaing the plot.

```{r sensitivity}
# this takes a while
for.plot = NULL
for (k in 1:10){ # repeat random selection 10 times
sens = filter(complete, mistake==FALSE, lower>0) %>% # exclude mistakes
  select(-journal, -Year, -mistake) %>% # tidy-up a bit
  group_by(pubmed) %>%
  mutate(rand = runif(n())) %>% # random number ...
  arrange(pubmed, rand) %>% # ... now sort by the random number and select the first result
  mutate(order=1:n())%>%
  ungroup() %>%
  filter(order==1) %>% # just one result per abstract
  select(-order) %>%
  mutate(k=k)
# now split into the lower and upper interval
lower = mutate(sens, itype='Lower') %>%
  select(k, itype, lower) %>%
  rename('interval'='lower') 
upper = mutate(sens, itype='Upper') %>%
  select(k, itype, upper) %>%
  rename('interval'='upper') 
for.plot = bind_rows(for.plot, lower, upper)
}
# plot the results
splot = ggplot(for.plot, aes(x=interval, lty=factor(k), col=factor(k)))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  theme_bw()+
  theme(panel.grid.minor = element_blank(), legend.position = 'none')+
  facet_wrap( ~ itype)
splot
```

The lines were completely overlapping, showing no impact of the repeated results.


# Unbiased data from Schuemie et al

```{r load.unbiased, include=FALSE}
# data from Figshare https://figshare.com/articles/Data_file_S1_from_Improving_reproducibility_by_using_high-throughput_observational_studies_with_empirical_calibration/6684080
unbiased = read.csv('data/rsta20170356_si_002.csv', stringsAsFactors = FALSE)
unbiased = dplyr::select(unbiased, True.hazard.ratio,  Hazard.ratio, Lower.bound.95..CI, Upper.bound.95..CI) %>%
  filter(True.hazard.ratio <=1.5) %>% # just the negative and one example of a positive association
  mutate(group = ifelse(True.hazard.ratio==1, 'Negative', 'Positive'))
```

Here we examine the data from Schuemie et al which is a large unbiased sample of hazard ratios from an observational analysis of insurance data.
We use `r format(nrow(unbiased),big.mark=',')` hazard ratios.

```{r unbiased.plot}
# add sample size to label
tab = table(unbiased$group)
labels = paste(names(tab), ' (n = ', format(as.numeric(tab), big.mark=','), ')', sep='')
unbiased =mutate(unbiased, group.nice = factor(group, levels=c('Negative', 'Positive'), labels=labels))
# split into the lower and upper interval
lower = mutate(unbiased, itype='Lower') %>%
  filter(is.na(Lower.bound.95..CI)==FALSE) %>%
  select(itype, True.hazard.ratio, Lower.bound.95..CI, group.nice) %>%
  rename('interval'='Lower.bound.95..CI') 
upper = mutate(unbiased, itype='Upper') %>%
  filter(is.na(Upper.bound.95..CI)==FALSE) %>%
  select(itype, True.hazard.ratio, Upper.bound.95..CI, group.nice) %>%
  rename('interval'='Upper.bound.95..CI') 
for.plot = bind_rows(lower, upper)
# plot
uplot = ggplot(for.plot, aes(x=interval, col=itype))+
  geom_vline(xintercept = 1, lty=1, col='black')+
  stat_ecdf(geom = "step", size=1.1)+
  coord_cartesian(xlim=c(0.1, 10))+ # limit x-axis to focus on key change
  scale_x_log10(breaks=c(0.1,0.5,1,2,10), labels=c(0.1,0.5,1,2,10))+
  scale_color_manual('Interval', values=cbPalette[2:3])+
  xlab('Ratio')+
  ylab('Cumulative distribution')+
  theme_bw()+
  theme(panel.grid.minor = element_blank(), legend.position = c(0.1,0.85))+
  facet_wrap( ~ group.nice)
uplot
# export plot
jpeg('figures/IntervalsSchuemie.jpg', width=5, height=4, units='in', res=400, quality=100)
print(uplot)
invisible(dev.off())
```

